#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Benchmark PySpark SQL vs DataFrame API - HDBank
==============================================
Script t·∫°o d·ªØ li·ªáu m·∫´u l·ªõn v√† so s√°nh hi·ªáu su·∫•t gi·ªØa SQL v√† DataFrame API
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, expr, rand, when, lit, desc, count, sum, current_timestamp
import time
import random
import string
import argparse
import os
from faker import Faker

# Thi·∫øt l·∫≠p Faker ƒë·ªÉ t·∫°o d·ªØ li·ªáu ng·∫´u nhi√™n
fake = Faker('vi_VN')

def create_spark_session(jdbc_jar_path):
    """Kh·ªüi t·∫°o SparkSession"""
    print("üöÄ ƒêang kh·ªüi t·∫°o SparkSession...")
    
    spark = SparkSession.builder \
        .appName("HDBank_PySpark_Benchmark") \
        .config("spark.jars", jdbc_jar_path) \
        .config("spark.sql.shuffle.partitions", "8") \
        .config("spark.default.parallelism", "8") \
        .config("spark.sql.adaptive.enabled", "true") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("ERROR")
    print("‚úÖ SparkSession ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o!")
    
    return spark

def generate_customer_data(spark, num_rows=10000):
    """T·∫°o d·ªØ li·ªáu kh√°ch h√†ng m·∫´u"""
    print(f"üìä T·∫°o {num_rows} kh√°ch h√†ng m·∫´u...")
    
    # T·∫°o danh s√°ch kh√°ch h√†ng
    customer_data = []
    cities = ["TP.HCM", "H√† N·ªôi", "ƒê√† N·∫µng", "C·∫ßn Th∆°", "H·∫£i Ph√≤ng", "Nha Trang", "Hu·∫ø", "V≈©ng T√†u"]
    genders = ["Nam", "N·ªØ"]
    
    for i in range(1, num_rows + 1):
        customer_data.append((
            i,
            fake.name(),
            random.randint(18, 70),
            random.choice(genders),
            random.choice(cities),
            f"0{random.randint(900000000, 999999999)}",
            random.randint(1000000, 1000000000)
        ))
    
    # T·∫°o DataFrame
    customers_columns = ["id", "name", "age", "gender", "city", "phone", "balance"]
    customers_df = spark.createDataFrame(customer_data, customers_columns)
    
    print(f"‚úÖ ƒê√£ t·∫°o {num_rows} kh√°ch h√†ng m·∫´u!")
    return customers_df

def generate_transaction_data(spark, num_customers, transactions_per_customer=10):
    """T·∫°o d·ªØ li·ªáu giao d·ªãch m·∫´u"""
    num_rows = num_customers * transactions_per_customer
    print(f"üìä T·∫°o {num_rows} giao d·ªãch m·∫´u...")
    
    # T·∫°o danh s√°ch giao d·ªãch
    transaction_data = []
    transaction_types = ["deposit", "withdrawal", "transfer", "payment"]
    descriptions = [
        "Ti·ªÅn l∆∞∆°ng", "R√∫t ti·ªÅn m·∫∑t", "Chuy·ªÉn ti·ªÅn", "Ti·∫øt ki·ªám", 
        "Thanh to√°n h√≥a ƒë∆°n", "ƒê·∫ßu t∆∞", "Nh·∫≠n ti·ªÅn", "Tr·∫£ n·ª£",
        "Ti·ªÅn th∆∞·ªüng", "Ti·ªÅn l√£i"
    ]
    
    transaction_id = 1
    for i in range(1, num_customers + 1):
        for j in range(transactions_per_customer):
            transaction_type = random.choice(transaction_types)
            
            # T·∫°o s·ªë ti·ªÅn ph√π h·ª£p v·ªõi lo·∫°i giao d·ªãch
            if transaction_type == "deposit":
                amount = random.randint(1000000, 50000000)
            elif transaction_type == "withdrawal":
                amount = random.randint(500000, 10000000)
            elif transaction_type == "transfer":
                amount = random.randint(100000, 20000000)
            else:  # payment
                amount = random.randint(50000, 5000000)
            
            # T·∫°o ng√†y giao d·ªãch
            year = 2023
            month = random.randint(1, 12)
            day = random.randint(1, 28)
            date = f"{year}-{month:02d}-{day:02d}"
            
            transaction_data.append((
                transaction_id,
                i,  # customer_id
                date,
                transaction_type,
                amount,
                random.choice(descriptions)
            ))
            
            transaction_id += 1
    
    # T·∫°o DataFrame
    transactions_columns = ["id", "customer_id", "date", "type", "amount", "description"]
    transactions_df = spark.createDataFrame(transaction_data, transactions_columns)
    
    print(f"‚úÖ ƒê√£ t·∫°o {num_rows} giao d·ªãch m·∫´u!")
    return transactions_df

def benchmark_sql_vs_dataframe(spark, customers_df, transactions_df, iterations=3):
    """So s√°nh hi·ªáu su·∫•t gi·ªØa SQL v√† DataFrame API"""
    print("\nüìä B·∫Øt ƒë·∫ßu benchmark SQL vs DataFrame API...")
    
    # ƒêƒÉng k√Ω DataFrames nh∆∞ c√°c b·∫£ng t·∫°m ƒë·ªÉ truy v·∫•n SQL
    customers_df.createOrReplaceTempView("customers")
    transactions_df.createOrReplaceTempView("transactions")
    
    # Kh·ªüi t·∫°o k·∫øt qu·∫£ benchmark
    sql_times = []
    df_times = []
    
    for i in range(iterations):
        print(f"\nüîÑ L·∫ßn ch·∫°y #{i+1}")
        
        # ------ PH·∫¶N 1: TRUY V·∫§N SQL ------
        print("\nüìä [SQL] Th·ª±c hi·ªán ph√¢n t√≠ch giao d·ªãch...")
        sql_start_time = time.time()
        
        sql_query = """
            SELECT 
                c.id, 
                c.name, 
                c.city, 
                c.gender,
                COUNT(t.id) as transaction_count,
                SUM(CASE WHEN t.type = 'deposit' THEN t.amount ELSE 0 END) as total_deposits,
                SUM(CASE WHEN t.type = 'withdrawal' THEN t.amount ELSE 0 END) as total_withdrawals,
                SUM(CASE WHEN t.type = 'transfer' THEN t.amount ELSE 0 END) as total_transfers,
                SUM(CASE WHEN t.type = 'payment' THEN t.amount ELSE 0 END) as total_payments,
                SUM(CASE WHEN t.type = 'deposit' THEN t.amount 
                     WHEN t.type IN ('withdrawal', 'payment', 'transfer') THEN -t.amount 
                     ELSE 0 END) as net_flow,
                MIN(t.date) as first_transaction_date,
                MAX(t.date) as last_transaction_date
            FROM customers c
            LEFT JOIN transactions t ON c.id = t.customer_id
            GROUP BY c.id, c.name, c.city, c.gender
            ORDER BY net_flow DESC
        """
        
        sql_result = spark.sql(sql_query)
        # Th·ª±c hi·ªán action ƒë·ªÉ ƒë·∫£m b·∫£o truy v·∫•n ƒë∆∞·ª£c th·ª±c thi
        sql_result_count = sql_result.count()
        
        sql_execution_time = time.time() - sql_start_time
        sql_times.append(sql_execution_time)
        print(f"‚è±Ô∏è Th·ªùi gian th·ª±c thi SQL: {sql_execution_time:.4f} gi√¢y")
        print(f"üìä K·∫øt qu·∫£: {sql_result_count} d√≤ng")
        
        # Hi·ªÉn th·ªã 5 k·∫øt qu·∫£ ƒë·∫ßu ti√™n
        print("M·∫´u k·∫øt qu·∫£:")
        sql_result.show(5, truncate=False)
        
        # ------ PH·∫¶N 2: DATAFRAME API ------
        print("\nüìä [DataFrame API] Th·ª±c hi·ªán ph√¢n t√≠ch giao d·ªãch...")
        df_start_time = time.time()
        
        from pyspark.sql.functions import min, max
        
        df_result = customers_df.join(
            transactions_df,
            customers_df.id == transactions_df.customer_id,
            "left"
        ).groupBy(
            customers_df.id,
            customers_df.name,
            customers_df.city,
            customers_df.gender
        ).agg(
            count(transactions_df.id).alias("transaction_count"),
            sum(when(transactions_df.type == "deposit", transactions_df.amount).otherwise(0)).alias("total_deposits"),
            sum(when(transactions_df.type == "withdrawal", transactions_df.amount).otherwise(0)).alias("total_withdrawals"),
            sum(when(transactions_df.type == "transfer", transactions_df.amount).otherwise(0)).alias("total_transfers"),
            sum(when(transactions_df.type == "payment", transactions_df.amount).otherwise(0)).alias("total_payments"),
            sum(when(transactions_df.type == "deposit", transactions_df.amount)
                .when(transactions_df.type.isin("withdrawal", "payment", "transfer"), -transactions_df.amount)
                .otherwise(0)).alias("net_flow"),
            min(transactions_df.date).alias("first_transaction_date"),
            max(transactions_df.date).alias("last_transaction_date")
        ).orderBy(desc("net_flow"))
        
        # Th·ª±c hi·ªán action ƒë·ªÉ ƒë·∫£m b·∫£o truy v·∫•n ƒë∆∞·ª£c th·ª±c thi
        df_result_count = df_result.count()
        
        df_execution_time = time.time() - df_start_time
        df_times.append(df_execution_time)
        print(f"‚è±Ô∏è Th·ªùi gian th·ª±c thi DataFrame API: {df_execution_time:.4f} gi√¢y")
        print(f"üìä K·∫øt qu·∫£: {df_result_count} d√≤ng")
        
        # Hi·ªÉn th·ªã 5 k·∫øt qu·∫£ ƒë·∫ßu ti√™n
        print("M·∫´u k·∫øt qu·∫£:")
        df_result.show(5, truncate=False)
    
    # T√≠nh trung b√¨nh c√°c l·∫ßn ch·∫°y
    avg_sql_time = sum(sql_times) / len(sql_times)
    avg_df_time = sum(df_times) / len(df_times)
    
    # Hi·ªÉn th·ªã k·∫øt qu·∫£ t·ªïng h·ª£p
    print("\nüìà K·∫æT QU·∫¢ BENCHMARK T·ªîNG H·ª¢P:")
    print(f"S·ªë l∆∞·ª£ng kh√°ch h√†ng: {customers_df.count()}")
    print(f"S·ªë l∆∞·ª£ng giao d·ªãch: {transactions_df.count()}")
    print(f"S·ªë l·∫ßn ch·∫°y: {iterations}")
    print("\nTh·ªùi gian th·ª±c thi trung b√¨nh:")
    print(f"   SQL:          {avg_sql_time:.4f} gi√¢y")
    print(f"   DataFrame:    {avg_df_time:.4f} gi√¢y")
    print(f"   Ch√™nh l·ªách:   {abs(avg_sql_time - avg_df_time):.4f} gi√¢y")
    
    if avg_sql_time < avg_df_time:
        print(f"   üëâ SQL nhanh h∆°n {(avg_df_time/avg_sql_time - 1)*100:.2f}%")
    else:
        print(f"   üëâ DataFrame API nhanh h∆°n {(avg_sql_time/avg_df_time - 1)*100:.2f}%")
    
    return {
        "sql_times": sql_times,
        "df_times": df_times,
        "avg_sql_time": avg_sql_time,
        "avg_df_time": avg_df_time
    }

def write_to_postgres(customers_df, transactions_df, jdbc_url, connection_properties):
    """Ghi d·ªØ li·ªáu v√†o PostgreSQL"""
    print("\nüì• ƒêang ghi d·ªØ li·ªáu v√†o PostgreSQL...")
    
    # Ghi d·ªØ li·ªáu kh√°ch h√†ng
    print("üìä Ghi b·∫£ng customers...")
    start_time = time.time()
    customers_df.write \
        .mode("overwrite") \
        .jdbc(jdbc_url, "benchmark_customers", properties=connection_properties)
    print(f"‚úÖ Ho√†n th√†nh trong {time.time() - start_time:.2f} gi√¢y")
    
    # Ghi d·ªØ li·ªáu giao d·ªãch
    print("üìä Ghi b·∫£ng transactions...")
    start_time = time.time()
    transactions_df.write \
        .mode("overwrite") \
        .jdbc(jdbc_url, "benchmark_transactions", properties=connection_properties)
    print(f"‚úÖ Ho√†n th√†nh trong {time.time() - start_time:.2f} gi√¢y")
    
    print("‚úÖ ƒê√£ ghi d·ªØ li·ªáu v√†o PostgreSQL th√†nh c√¥ng!")

def benchmark_postgres_vs_spark(spark, jdbc_url, connection_properties, iterations=3):
    """So s√°nh hi·ªáu su·∫•t truy v·∫•n gi·ªØa PostgreSQL v√† PySpark"""
    print("\nüìä B·∫Øt ƒë·∫ßu benchmark PostgreSQL vs PySpark...")
    
    # Truy v·∫•n tr·ª±c ti·∫øp t·ª´ PostgreSQL
    postgres_times = []
    spark_times = []
    
    for i in range(iterations):
        print(f"\nüîÑ L·∫ßn ch·∫°y #{i+1}")
        
        # ------ PH·∫¶N 1: TRUY V·∫§N POSTGRESQL ------
        print("\nüìä [PostgreSQL] Th·ª±c hi·ªán truy v·∫•n tr·ª±c ti·∫øp t·ª´ PostgreSQL...")
        pg_start_time = time.time()
        
        # S·ª≠ d·ª•ng pushdown query ƒë·ªÉ th·ª±c thi truy v·∫•n ho√†n to√†n trong PostgreSQL
        pushdown_query = """
        (SELECT 
            c.id, 
            c.name, 
            c.city, 
            c.gender,
            COUNT(t.id) as transaction_count,
            SUM(CASE WHEN t.type = 'deposit' THEN t.amount ELSE 0 END) as total_deposits,
            SUM(CASE WHEN t.type = 'withdrawal' THEN t.amount ELSE 0 END) as total_withdrawals,
            SUM(CASE WHEN t.type = 'transfer' THEN t.amount ELSE 0 END) as total_transfers,
            SUM(CASE WHEN t.type = 'payment' THEN t.amount ELSE 0 END) as total_payments,
            SUM(CASE WHEN t.type = 'deposit' THEN t.amount 
                 WHEN t.type IN ('withdrawal', 'payment', 'transfer') THEN -t.amount 
                 ELSE 0 END) as net_flow,
            MIN(t.date) as first_transaction_date,
            MAX(t.date) as last_transaction_date
        FROM benchmark_customers c
        LEFT JOIN benchmark_transactions t ON c.id = t.customer_id
        GROUP BY c.id, c.name, c.city, c.gender
        ORDER BY net_flow DESC) AS query_result
        """
        
        pg_result = spark.read \
            .jdbc(jdbc_url, pushdown_query, properties=connection_properties)
        
        # Th·ª±c hi·ªán action ƒë·ªÉ ƒë·∫£m b·∫£o truy v·∫•n ƒë∆∞·ª£c th·ª±c thi
        pg_result_count = pg_result.count()
        
        pg_execution_time = time.time() - pg_start_time
        postgres_times.append(pg_execution_time)
        print(f"‚è±Ô∏è Th·ªùi gian truy v·∫•n PostgreSQL: {pg_execution_time:.4f} gi√¢y")
        print(f"üìä K·∫øt qu·∫£: {pg_result_count} d√≤ng")
        
        # Hi·ªÉn th·ªã 5 k·∫øt qu·∫£ ƒë·∫ßu ti√™n
        print("M·∫´u k·∫øt qu·∫£:")
        pg_result.show(5, truncate=False)
        
        # ------ PH·∫¶N 2: TRUY V·∫§N SPARK ------
        print("\nüìä [Spark] ƒê·ªçc d·ªØ li·ªáu t·ª´ PostgreSQL v√† x·ª≠ l√Ω trong PySpark...")
        spark_start_time = time.time()
        
        # ƒê·ªçc d·ªØ li·ªáu t·ª´ PostgreSQL v√†o Spark
        customers_df = spark.read \
            .jdbc(jdbc_url, "benchmark_customers", properties=connection_properties)
        
        transactions_df = spark.read \
            .jdbc(jdbc_url, "benchmark_transactions", properties=connection_properties)
        
        # ƒêƒÉng k√Ω DataFrames nh∆∞ c√°c b·∫£ng t·∫°m ƒë·ªÉ truy v·∫•n SQL
        customers_df.createOrReplaceTempView("customers")
        transactions_df.createOrReplaceTempView("transactions")
        
        # Th·ª±c hi·ªán truy v·∫•n trong Spark
        spark_query = """
            SELECT 
                c.id, 
                c.name, 
                c.city, 
                c.gender,
                COUNT(t.id) as transaction_count,
                SUM(CASE WHEN t.type = 'deposit' THEN t.amount ELSE 0 END) as total_deposits,
                SUM(CASE WHEN t.type = 'withdrawal' THEN t.amount ELSE 0 END) as total_withdrawals,
                SUM(CASE WHEN t.type = 'transfer' THEN t.amount ELSE 0 END) as total_transfers,
                SUM(CASE WHEN t.type = 'payment' THEN t.amount ELSE 0 END) as total_payments,
                SUM(CASE WHEN t.type = 'deposit' THEN t.amount 
                     WHEN t.type IN ('withdrawal', 'payment', 'transfer') THEN -t.amount 
                     ELSE 0 END) as net_flow,
                MIN(t.date) as first_transaction_date,
                MAX(t.date) as last_transaction_date
            FROM customers c
            LEFT JOIN transactions t ON c.id = t.customer_id
            GROUP BY c.id, c.name, c.city, c.gender
            ORDER BY net_flow DESC
        """
        
        spark_result = spark.sql(spark_query)
        
        # Th·ª±c hi·ªán action ƒë·ªÉ ƒë·∫£m b·∫£o truy v·∫•n ƒë∆∞·ª£c th·ª±c thi
        spark_result_count = spark_result.count()
        
        spark_execution_time = time.time() - spark_start_time
        spark_times.append(spark_execution_time)
        print(f"‚è±Ô∏è Th·ªùi gian x·ª≠ l√Ω PySpark: {spark_execution_time:.4f} gi√¢y")
        print(f"üìä K·∫øt qu·∫£: {spark_result_count} d√≤ng")
        
        # Hi·ªÉn th·ªã 5 k·∫øt qu·∫£ ƒë·∫ßu ti√™n
        print("M·∫´u k·∫øt qu·∫£:")
        spark_result.show(5, truncate=False)
    
    # T√≠nh trung b√¨nh c√°c l·∫ßn ch·∫°y
    avg_postgres_time = sum(postgres_times) / len(postgres_times)
    avg_spark_time = sum(spark_times) / len(spark_times)
    
    # Hi·ªÉn th·ªã k·∫øt qu·∫£ t·ªïng h·ª£p
    print("\nüìà K·∫æT QU·∫¢ BENCHMARK POSTGRESQL VS PYSPARK:")
    print(f"S·ªë l·∫ßn ch·∫°y: {iterations}")
    print("\nTh·ªùi gian th·ª±c thi trung b√¨nh:")
    print(f"   PostgreSQL:   {avg_postgres_time:.4f} gi√¢y")
    print(f"   PySpark:      {avg_spark_time:.4f} gi√¢y")
    print(f"   Ch√™nh l·ªách:   {abs(avg_postgres_time - avg_spark_time):.4f} gi√¢y")
    
    if avg_postgres_time < avg_spark_time:
        print(f"   üëâ PostgreSQL nhanh h∆°n {(avg_spark_time/avg_postgres_time - 1)*100:.2f}%")
    else:
        print(f"   üëâ PySpark nhanh h∆°n {(avg_postgres_time/avg_spark_time - 1)*100:.2f}%")
    
    return {
        "postgres_times": postgres_times,
        "spark_times": spark_times,
        "avg_postgres_time": avg_postgres_time,
        "avg_spark_time": avg_spark_time
    }

def main():
    parser = argparse.ArgumentParser(description='HDBank PySpark SQL vs DataFrame Benchmark')
    parser.add_argument('--jar', type=str, 
                        default="/home/devduongthanhdat/Desktop/hdbank/hdbank_pyspark/postgresql-42.7.2.jar",
                        help='ƒê∆∞·ªùng d·∫´n t·ªõi PostgreSQL JDBC JAR')
    parser.add_argument('--db', type=str, default="hdbank_db", 
                        help='T√™n database PostgreSQL')
    parser.add_argument('--user', type=str, default="postgres", 
                        help='Username PostgreSQL')
    parser.add_argument('--password', type=str, default="postgres", 
                        help='M·∫≠t kh·∫©u PostgreSQL')
    parser.add_argument('--customers', type=int, default=1000, 
                        help='S·ªë l∆∞·ª£ng kh√°ch h√†ng ƒë·ªÉ t·∫°o')
    parser.add_argument('--transactions', type=int, default=10, 
                        help='S·ªë l∆∞·ª£ng giao d·ªãch tr√™n m·ªói kh√°ch h√†ng')
    parser.add_argument('--iterations', type=int, default=3, 
                        help='S·ªë l·∫ßn ch·∫°y m·ªói benchmark')
    parser.add_argument('--write-to-postgres', action='store_true',
                        help='Ghi d·ªØ li·ªáu v√†o PostgreSQL ƒë·ªÉ benchmark')
    parser.add_argument('--postgres-vs-spark', action='store_true',
                        help='So s√°nh hi·ªáu su·∫•t gi·ªØa PostgreSQL v√† PySpark')
    
    args = parser.parse_args()
    
    # Thi·∫øt l·∫≠p th√¥ng tin k·∫øt n·ªëi JDBC
    jdbc_url = f"jdbc:postgresql://localhost:5432/{args.db}"
    connection_properties = {
        "user": args.user,
        "password": args.password,
        "driver": "org.postgresql.Driver"
    }
    
    try:
        print("\nüè¶ HDBANK: BENCHMARK PYSPARK SQL VS DATAFRAME API")
        print("=" * 60)
        
        # Ki·ªÉm tra file JAR t·ªìn t·∫°i
        if not os.path.exists(args.jar):
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y JDBC JAR t·∫°i: {args.jar}")
        
        # T·∫°o Spark session
        spark = create_spark_session(args.jar)
        
        # T·∫°o d·ªØ li·ªáu m·∫´u
        customers_df = generate_customer_data(spark, args.customers)
        transactions_df = generate_transaction_data(spark, args.customers, args.transactions)
        
        # Benchmark SQL vs DataFrame API
        benchmark_results = benchmark_sql_vs_dataframe(spark, customers_df, transactions_df, args.iterations)
        
        # N·∫øu c·∫ßn ghi v√†o PostgreSQL v√† benchmark
        if args.write_to_postgres:
            write_to_postgres(customers_df, transactions_df, jdbc_url, connection_properties)
            
            if args.postgres_vs_spark:
                pg_spark_results = benchmark_postgres_vs_spark(spark, jdbc_url, connection_properties, args.iterations)
        
        print("\nüéâ Benchmark ho√†n t·∫•t!")
        
    except Exception as e:
        print(f"‚ùå L·ªói: {str(e)}")
        import traceback
        traceback.print_exc()
        
    finally:
        if 'spark' in locals():
            spark.stop()
            print("‚úÖ ƒê√£ ƒë√≥ng SparkSession")

if __name__ == "__main__":
    main()
