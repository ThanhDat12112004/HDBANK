# -*- coding: utf-8 -*-
"""
K·∫øt n·ªëi PySpark v·ªõi PostgreSQL th√¥ng qua JDBC
T√°c gi·∫£: HDBank Data Team
Ng√†y: 27/05/2025
"""

from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType
import time

def create_spark_session():
    """T·∫°o SparkSession v·ªõi JDBC driver cho PostgreSQL"""
    print("üöÄ ƒêang kh·ªüi t·∫°o SparkSession v·ªõi PostgreSQL JDBC...")
    
    spark = SparkSession.builder \
        .appName("HDBank_PostgreSQL_JDBC") \
        .config("spark.jars", "/home/devduongthanhdat/Desktop/hdbank/hdbank_pyspark/postgresql-42.7.2.jar") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("ERROR")
    print("‚úÖ SparkSession ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o th√†nh c√¥ng!")
    return spark

def create_jdbc_connection_properties():
    """T·∫°o th√¥ng tin k·∫øt n·ªëi JDBC"""
    return {
        "url": "jdbc:postgresql://localhost:5432/hdbank_db",
        "properties": {
            "user": "postgres",
            "password": "hdbank123",
            "driver": "org.postgresql.Driver"
        }
    }

def create_sample_data(spark):
    """T·∫°o d·ªØ li·ªáu m·∫´u cho demo"""
    print("üìä ƒêang t·∫°o d·ªØ li·ªáu m·∫´u...")
    
    # T·∫°o d·ªØ li·ªáu kh√°ch h√†ng HDBank
    customer_data = [
        (1, "Nguy·ªÖn VƒÉn An", "0901234567", "HCM", 25, 15000000.0, "VIP"),
        (2, "Tr·∫ßn Th·ªã B√¨nh", "0907654321", "HN", 30, 25000000.0, "GOLD"),
        (3, "L√™ VƒÉn C∆∞·ªùng", "0903456789", "DN", 28, 8000000.0, "SILVER"),
        (4, "Ph·∫°m Th·ªã Dung", "0905678901", "CT", 35, 50000000.0, "DIAMOND"),
        (5, "Ho√†ng VƒÉn Em", "0902345678", "HCM", 22, 3000000.0, "BASIC"),
        (6, "V≈© Th·ªã F∆∞∆°ng", "0908765432", "HN", 27, 12000000.0, "SILVER"),
        (7, "ƒê·ªó VƒÉn Giang", "0904567890", "HP", 33, 35000000.0, "VIP"),
        (8, "B√πi Th·ªã H·∫°nh", "0906789012", "HCM", 29, 18000000.0, "GOLD"),
        (9, "Ng√¥ VƒÉn √çch", "0901357924", "DN", 31, 22000000.0, "GOLD"),
        (10, "Tr∆∞∆°ng Th·ªã Kim", "0907531864", "CT", 26, 6000000.0, "SILVER")
    ]
    
    schema = StructType([
        StructField("customer_id", IntegerType(), True),
        StructField("customer_name", StringType(), True),
        StructField("phone", StringType(), True),
        StructField("city", StringType(), True),
        StructField("age", IntegerType(), True),
        StructField("balance", DoubleType(), True),
        StructField("tier", StringType(), True)
    ])
    
    df = spark.createDataFrame(customer_data, schema)
    print(f"‚úÖ ƒê√£ t·∫°o {df.count()} b·∫£n ghi kh√°ch h√†ng")
    return df

def write_to_postgresql(df, table_name, jdbc_config):
    """Ghi DataFrame v√†o PostgreSQL"""
    print(f"üìù ƒêang ghi d·ªØ li·ªáu v√†o b·∫£ng '{table_name}'...")
    start_time = time.time()
    
    try:
        df.write \
            .mode("overwrite") \
            .option("driver", jdbc_config["properties"]["driver"]) \
            .jdbc(
                url=jdbc_config["url"],
                table=table_name,
                properties=jdbc_config["properties"]
            )
        
        write_time = time.time() - start_time
        print(f"‚úÖ Ghi d·ªØ li·ªáu th√†nh c√¥ng! Th·ªùi gian: {write_time:.2f} gi√¢y")
        
    except Exception as e:
        print(f"‚ùå L·ªói khi ghi d·ªØ li·ªáu: {str(e)}")

def read_from_postgresql(spark, table_name, jdbc_config):
    """ƒê·ªçc d·ªØ li·ªáu t·ª´ PostgreSQL"""
    print(f"üìñ ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´ b·∫£ng '{table_name}'...")
    start_time = time.time()
    
    try:
        df = spark.read \
            .option("driver", jdbc_config["properties"]["driver"]) \
            .jdbc(
                url=jdbc_config["url"],
                table=table_name,
                properties=jdbc_config["properties"]
            )
        
        read_time = time.time() - start_time
        print(f"‚úÖ ƒê·ªçc d·ªØ li·ªáu th√†nh c√¥ng! Th·ªùi gian: {read_time:.2f} gi√¢y")
        return df
        
    except Exception as e:
        print(f"‚ùå L·ªói khi ƒë·ªçc d·ªØ li·ªáu: {str(e)}")
        return None

def perform_analysis(spark, df):
    """Th·ª±c hi·ªán ph√¢n t√≠ch d·ªØ li·ªáu"""
    print("\nüîç === PH√ÇN T√çCH D·ªÆ LI·ªÜU KH√ÅCH H√ÄNG HDBANK ===")
    
    # T·∫°o temp view ƒë·ªÉ th·ª±c hi·ªán SQL
    df.createOrReplaceTempView("customers")
    
    print("\n1. üìä Th·ªëng k√™ t·ªïng quan:")
    df.show()
    
    print("\n2. üí∞ Ph√¢n t√≠ch theo tier kh√°ch h√†ng:")
    tier_analysis = spark.sql("""
        SELECT 
            tier,
            COUNT(*) as so_khach_hang,
            AVG(balance) as so_du_trung_binh,
            AVG(age) as tuoi_trung_binh
        FROM customers 
        GROUP BY tier 
        ORDER BY so_du_trung_binh DESC
    """)
    tier_analysis.show()
    
    print("\n3. üèôÔ∏è Ph√¢n t√≠ch theo th√†nh ph·ªë:")
    city_analysis = spark.sql("""
        SELECT 
            city,
            COUNT(*) as so_khach_hang,
            SUM(balance) as tong_so_du,
            AVG(balance) as so_du_trung_binh
        FROM customers 
        GROUP BY city 
        ORDER BY tong_so_du DESC
    """)
    city_analysis.show()
    
    print("\n4. üéØ Top 5 kh√°ch h√†ng c√≥ s·ªë d∆∞ cao nh·∫•t:")
    top_customers = spark.sql("""
        SELECT customer_name, city, balance, tier
        FROM customers 
        ORDER BY balance DESC 
        LIMIT 5
    """)
    top_customers.show()
    
    print("\n5. üìà Th·ªëng k√™ theo ƒë·ªô tu·ªïi:")
    age_stats = spark.sql("""
        SELECT 
            CASE 
                WHEN age < 25 THEN 'D∆∞·ªõi 25'
                WHEN age BETWEEN 25 AND 30 THEN '25-30'
                WHEN age BETWEEN 31 AND 35 THEN '31-35'
                ELSE 'Tr√™n 35'
            END as nhom_tuoi,
            COUNT(*) as so_luong,
            AVG(balance) as so_du_trung_binh
        FROM customers 
        GROUP BY 
            CASE 
                WHEN age < 25 THEN 'D∆∞·ªõi 25'
                WHEN age BETWEEN 25 AND 30 THEN '25-30'
                WHEN age BETWEEN 31 AND 35 THEN '31-35'
                ELSE 'Tr√™n 35'
            END
        ORDER BY so_du_trung_binh DESC
    """)
    age_stats.show()

def test_jdbc_performance(spark, jdbc_config):
    """Test hi·ªáu su·∫•t JDBC v·ªõi d·ªØ li·ªáu l·ªõn h∆°n"""
    print("\n‚ö° === TEST HI·ªÜU SU·∫§T JDBC ===")
    
    # T·∫°o d·ªØ li·ªáu l·ªõn h∆°n
    print("üìä T·∫°o d·ªØ li·ªáu test v·ªõi 10,000 b·∫£n ghi...")
    large_data = []
    for i in range(10000):
        large_data.append((
            i + 1,
            f"Customer_{i+1}",
            f"090{i%9000000:07d}",
            ["HCM", "HN", "DN", "CT", "HP"][i % 5],
            20 + (i % 40),
            1000000.0 + (i * 1000),
            ["BASIC", "SILVER", "GOLD", "VIP", "DIAMOND"][i % 5]
        ))
    
    schema = StructType([
        StructField("customer_id", IntegerType(), True),
        StructField("customer_name", StringType(), True),
        StructField("phone", StringType(), True),
        StructField("city", StringType(), True),
        StructField("age", IntegerType(), True),
        StructField("balance", DoubleType(), True),
        StructField("tier", StringType(), True)
    ])
    
    large_df = spark.createDataFrame(large_data, schema)
    
    # Test write performance
    print("\nüìù Test hi·ªáu su·∫•t ghi d·ªØ li·ªáu l·ªõn...")
    write_to_postgresql(large_df, "customers_large", jdbc_config)
    
    # Test read performance
    print("\nüìñ Test hi·ªáu su·∫•t ƒë·ªçc d·ªØ li·ªáu l·ªõn...")
    read_df = read_from_postgresql(spark, "customers_large", jdbc_config)
    if read_df:
        print(f"‚úÖ ƒê√£ ƒë·ªçc {read_df.count()} b·∫£n ghi")

def main():
    """H√†m ch√≠nh"""
    print("üè¶ === HDBANK PYSPARK POSTGRESQL JDBC DEMO ===\n")
    
    # Kh·ªüi t·∫°o Spark
    spark = create_spark_session()
    
    # C·∫•u h√¨nh JDBC
    jdbc_config = create_jdbc_connection_properties()
    
    try:
        # T·∫°o d·ªØ li·ªáu m·∫´u
        df = create_sample_data(spark)
        
        # Ghi v√†o PostgreSQL
        write_to_postgresql(df, "customers", jdbc_config)
        
        # ƒê·ªçc t·ª´ PostgreSQL
        df_from_db = read_from_postgresql(spark, "customers", jdbc_config)
        
        if df_from_db:
            # Th·ª±c hi·ªán ph√¢n t√≠ch
            perform_analysis(spark, df_from_db)
            
            # Test hi·ªáu su·∫•t
            test_jdbc_performance(spark, jdbc_config)
        
        print("\nüéâ Demo ho√†n th√†nh th√†nh c√¥ng!")
        
    except Exception as e:
        print(f"‚ùå L·ªói trong qu√° tr√¨nh th·ª±c thi: {str(e)}")
        
    finally:
        # D·ªçn d·∫πp
        print("\nüßπ ƒêang d·ªçn d·∫πp resources...")
        spark.stop()
        print("‚úÖ SparkSession ƒë√£ ƒë∆∞·ª£c ƒë√≥ng")

if __name__ == "__main__":
    main()