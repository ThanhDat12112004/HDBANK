#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HDBank PySpark PostgreSQL JDBC Demo
===================================
Ch∆∞∆°ng tr√¨nh minh h·ªça t√≠ch h·ª£p PySpark v·ªõi PostgreSQL s·ª≠ d·ª•ng JDBC
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, expr, when, lit
import argparse
import time
import os

class PySparkPostgreSQLDemo:
    def __init__(self, jdbc_jar_path, db_name="hdbank_db", user="postgres", password="hdbank123"):
        """Kh·ªüi t·∫°o PySpark Session v√† thi·∫øt l·∫≠p k·∫øt n·ªëi PostgreSQL"""
        self.jdbc_jar_path = jdbc_jar_path
        self.db_name = db_name
        self.user = user
        self.password = password
        
        # Ki·ªÉm tra file JAR t·ªìn t·∫°i
        if not os.path.exists(jdbc_jar_path):
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y JDBC JAR t·∫°i: {jdbc_jar_path}")
        
        # Kh·ªüi t·∫°o SparkSession v·ªõi JDBC driver
        print("üöÄ ƒêang kh·ªüi t·∫°o SparkSession...")
        self.spark = SparkSession.builder \
            .appName("HDBank_PostgreSQL_JDBC_Demo") \
            .config("spark.jars", jdbc_jar_path) \
            .getOrCreate()
        
        # Gi·∫£m log level ƒë·ªÉ output d·ªÖ ƒë·ªçc h∆°n
        self.spark.sparkContext.setLogLevel("ERROR")
        
        # Thi·∫øt l·∫≠p th√¥ng tin k·∫øt n·ªëi JDBC
        self.jdbc_url = f"jdbc:postgresql://localhost:5432/{db_name}"
        self.connection_properties = {
            "user": user,
            "password": password,
            "driver": "org.postgresql.Driver"
        }
        
        print("‚úÖ SparkSession ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o!")
        print(f"üìä K·∫øt n·ªëi ƒë·∫øn PostgreSQL: {self.jdbc_url}")
    
    def create_demo_data(self):
        """T·∫°o d·ªØ li·ªáu m·∫´u: customers v√† transactions"""
        print("\nüìä T·∫°o d·ªØ li·ªáu m·∫´u...")
        
        # T·∫°o b·∫£ng kh√°ch h√†ng
        customers_data = [
            (1, "Nguy·ªÖn VƒÉn An", 25, "Nam", "TP.HCM", "0901234567", 15000000.0),
            (2, "Tr·∫ßn Th·ªã B√¨nh", 30, "N·ªØ", "H√† N·ªôi", "0918765432", 25000000.0),
            (3, "L√™ VƒÉn C∆∞·ªùng", 28, "Nam", "ƒê√† N·∫µng", "0897654321", 8000000.0),
            (4, "Ph·∫°m Th·ªã Dung", 35, "N·ªØ", "C·∫ßn Th∆°", "0976543210", 42000000.0),
            (5, "Ho√†ng VƒÉn Em", 40, "Nam", "TP.HCM", "0865432109", 67000000.0)
        ]
        customers_columns = ["id", "name", "age", "gender", "city", "phone", "balance"]
        customers_df = self.spark.createDataFrame(customers_data, customers_columns)
        
        # T·∫°o b·∫£ng giao d·ªãch
        transactions_data = [
            (101, 1, "2023-03-01", "deposit", 5000000.0, "Ti·ªÅn l∆∞∆°ng"),
            (102, 2, "2023-03-02", "withdrawal", 3000000.0, "R√∫t ti·ªÅn m·∫∑t"),
            (103, 1, "2023-03-03", "transfer", 2000000.0, "Chuy·ªÉn ti·ªÅn"),
            (104, 3, "2023-03-05", "deposit", 10000000.0, "Ti·∫øt ki·ªám"),
            (105, 2, "2023-03-07", "payment", 1500000.0, "Thanh to√°n h√≥a ƒë∆°n"),
            (106, 4, "2023-03-10", "deposit", 8000000.0, "Ti·ªÅn l∆∞∆°ng"),
            (107, 5, "2023-03-12", "withdrawal", 10000000.0, "R√∫t ti·ªÅn m·∫∑t"),
            (108, 3, "2023-03-15", "payment", 500000.0, "Thanh to√°n h√≥a ƒë∆°n"),
            (109, 1, "2023-03-18", "transfer", 3000000.0, "Chuy·ªÉn ti·ªÅn"),
            (110, 5, "2023-03-20", "deposit", 15000000.0, "ƒê·∫ßu t∆∞")
        ]
        transactions_columns = ["id", "customer_id", "date", "type", "amount", "description"]
        transactions_df = self.spark.createDataFrame(transactions_data, transactions_columns)
        
        # Ghi v√†o PostgreSQL
        try:
            customers_df.write \
                .mode("overwrite") \
                .jdbc(self.jdbc_url, "customers", properties=self.connection_properties)
                
            transactions_df.write \
                .mode("overwrite") \
                .jdbc(self.jdbc_url, "transactions", properties=self.connection_properties)
                
            print("‚úÖ T·∫°o d·ªØ li·ªáu m·∫´u th√†nh c√¥ng!")
            
        except Exception as e:
            print(f"‚ùå L·ªói khi t·∫°o d·ªØ li·ªáu m·∫´u: {str(e)}")
    
    def read_data(self, table_name):
        """ƒê·ªçc d·ªØ li·ªáu t·ª´ PostgreSQL"""
        try:
            print(f"\nüìñ ƒê·ªçc d·ªØ li·ªáu t·ª´ b·∫£ng '{table_name}'...")
            start_time = time.time()
            
            df = self.spark.read \
                .jdbc(self.jdbc_url, table_name, properties=self.connection_properties)
            
            print(f"‚úÖ ƒê·ªçc th√†nh c√¥ng trong {time.time() - start_time:.4f} gi√¢y!")
            print(f"üìã Schema c·ªßa b·∫£ng:")
            df.printSchema()
            print(f"üìä D·ªØ li·ªáu ({df.count()} d√≤ng):")
            df.show(truncate=False)
            
            return df
            
        except Exception as e:
            print(f"‚ùå L·ªói khi ƒë·ªçc d·ªØ li·ªáu: {str(e)}")
            return None

    def execute_sql_query(self, query, result_table="query_results"):
        """Th·ª±c thi truy v·∫•n SQL v√† l∆∞u k·∫øt qu·∫£ v√†o PostgreSQL"""
        try:
            print(f"\nüîç Th·ª±c thi truy v·∫•n SQL:")
            print(f"   {query}")
            
            # ƒêƒÉng k√Ω b·∫£ng t·∫°m ƒë·ªÉ truy v·∫•n SQL
            customers_df = self.spark.read \
                .jdbc(self.jdbc_url, "customers", properties=self.connection_properties)
            transactions_df = self.spark.read \
                .jdbc(self.jdbc_url, "transactions", properties=self.connection_properties)
            
            customers_df.createOrReplaceTempView("customers")
            transactions_df.createOrReplaceTempView("transactions")
            
            # Th·ª±c thi truy v·∫•n
            start_time = time.time()
            result_df = self.spark.sql(query)
            
            print(f"‚úÖ Truy v·∫•n ho√†n th√†nh trong {time.time() - start_time:.4f} gi√¢y!")
            print(f"üìä K·∫øt qu·∫£ ({result_df.count()} d√≤ng):")
            result_df.show(truncate=False)
            
            # L∆∞u k·∫øt qu·∫£ v√†o PostgreSQL
            result_df.write \
                .mode("overwrite") \
                .jdbc(self.jdbc_url, result_table, properties=self.connection_properties)
            print(f"‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o b·∫£ng '{result_table}'")
            
            return result_df
            
        except Exception as e:
            print(f"‚ùå L·ªói khi th·ª±c thi truy v·∫•n: {str(e)}")
            return None
    
    def update_data(self, table_name, condition_col, condition_value, update_col, update_value):
        """C·∫≠p nh·∫≠t d·ªØ li·ªáu trong PostgreSQL"""
        try:
            print(f"\nüîÑ C·∫≠p nh·∫≠t d·ªØ li·ªáu trong b·∫£ng '{table_name}'...")
            
            # ƒê·ªçc d·ªØ li·ªáu hi·ªán t·∫°i
            df = self.spark.read \
                .jdbc(self.jdbc_url, table_name, properties=self.connection_properties)
            
            # Hi·ªÉn th·ªã d·ªØ li·ªáu tr∆∞·ªõc khi c·∫≠p nh·∫≠t
            print("üìä D·ªØ li·ªáu tr∆∞·ªõc khi c·∫≠p nh·∫≠t:")
            df.filter(col(condition_col) == condition_value).show()
            
            # Th·ª±c hi·ªán c·∫≠p nh·∫≠t
            updated_df = df.withColumn(
                update_col,
                when(col(condition_col) == condition_value, update_value).otherwise(col(update_col))
            )
            
            # Ghi l·∫°i d·ªØ li·ªáu ƒë√£ c·∫≠p nh·∫≠t
            updated_df.write \
                .mode("overwrite") \
                .jdbc(self.jdbc_url, table_name, properties=self.connection_properties)
            
            # Hi·ªÉn th·ªã d·ªØ li·ªáu sau khi c·∫≠p nh·∫≠t
            print("‚úÖ C·∫≠p nh·∫≠t th√†nh c√¥ng!")
            print("üìä D·ªØ li·ªáu sau khi c·∫≠p nh·∫≠t:")
            updated_df.filter(col(condition_col) == condition_value).show()
            
        except Exception as e:
            print(f"‚ùå L·ªói khi c·∫≠p nh·∫≠t d·ªØ li·ªáu: {str(e)}")
    
    def delete_data(self, table_name, condition_col, condition_value):
        """X√≥a d·ªØ li·ªáu t·ª´ PostgreSQL"""
        try:
            print(f"\nüóëÔ∏è X√≥a d·ªØ li·ªáu t·ª´ b·∫£ng '{table_name}'...")
            
            # ƒê·ªçc d·ªØ li·ªáu hi·ªán t·∫°i
            df = self.spark.read \
                .jdbc(self.jdbc_url, table_name, properties=self.connection_properties)
            
            # Hi·ªÉn th·ªã d·ªØ li·ªáu tr∆∞·ªõc khi x√≥a
            print(f"üìä S·ªë d√≤ng tr∆∞·ªõc khi x√≥a: {df.count()}")
            print("üìä D·ªØ li·ªáu s·∫Ω b·ªã x√≥a:")
            df.filter(col(condition_col) == condition_value).show()
            
            # Th·ª±c hi·ªán x√≥a
            filtered_df = df.filter(col(condition_col) != condition_value)
            
            # Ghi l·∫°i d·ªØ li·ªáu ƒë√£ l·ªçc
            filtered_df.write \
                .mode("overwrite") \
                .jdbc(self.jdbc_url, table_name, properties=self.connection_properties)
            
            # Hi·ªÉn th·ªã th√¥ng tin sau khi x√≥a
            print("‚úÖ X√≥a th√†nh c√¥ng!")
            print(f"üìä S·ªë d√≤ng sau khi x√≥a: {filtered_df.count()}")
            
        except Exception as e:
            print(f"‚ùå L·ªói khi x√≥a d·ªØ li·ªáu: {str(e)}")
    
    def join_data_demonstration(self):
        """Demo thao t√°c join d·ªØ li·ªáu gi·ªØa c√°c b·∫£ng"""
        try:
            print("\nüîÑ Demo thao t√°c join d·ªØ li·ªáu...")
            
            # ƒê·ªçc d·ªØ li·ªáu t·ª´ c√°c b·∫£ng
            customers_df = self.spark.read \
                .jdbc(self.jdbc_url, "customers", properties=self.connection_properties)
            
            transactions_df = self.spark.read \
                .jdbc(self.jdbc_url, "transactions", properties=self.connection_properties)
            
            # ƒêƒÉng k√Ω DataFrames nh∆∞ c√°c b·∫£ng t·∫°m ƒë·ªÉ truy v·∫•n SQL
            customers_df.createOrReplaceTempView("customers")
            transactions_df.createOrReplaceTempView("transactions")
            
            # ------ PH·∫¶N 1: TRUY V·∫§N S·ª¨ D·ª§NG SQL ------
            print("\nüìä [SQL] T√≠nh t·ªïng giao d·ªãch theo t·ª´ng kh√°ch h√†ng:")
            sql_start_time = time.time()
            query = """
                SELECT 
                    c.id, 
                    c.name, 
                    c.city, 
                    COUNT(t.id) as transaction_count,
                    SUM(CASE WHEN t.type = 'deposit' THEN t.amount ELSE 0 END) as total_deposits,
                    SUM(CASE WHEN t.type IN ('withdrawal', 'payment', 'transfer') THEN t.amount ELSE 0 END) as total_debits,
                    SUM(CASE WHEN t.type = 'deposit' THEN t.amount ELSE -t.amount END) as net_flow
                FROM customers c
                LEFT JOIN transactions t ON c.id = t.customer_id
                GROUP BY c.id, c.name, c.city
                ORDER BY net_flow DESC
            """
            
            sql_result_df = self.spark.sql(query)
            sql_execution_time = time.time() - sql_start_time
            print(f"‚è±Ô∏è Th·ªùi gian th·ª±c thi SQL: {sql_execution_time:.4f} gi√¢y")
            sql_result_df.show(truncate=False)
            
            # ------ PH·∫¶N 2: TRUY V·∫§N S·ª¨ D·ª§NG DATAFRAME API ------
            print("\nüìä [DataFrame API] T√≠nh t·ªïng giao d·ªãch theo t·ª´ng kh√°ch h√†ng:")
            df_start_time = time.time()
            
            # Th·ª±c hi·ªán join b·∫±ng DataFrame API
            from pyspark.sql.functions import sum, count, when, desc
            
            df_result = customers_df.join(
                transactions_df,
                customers_df.id == transactions_df.customer_id,
                "left"
            ).groupBy(
                customers_df.id,
                customers_df.name,
                customers_df.city
            ).agg(
                count(transactions_df.id).alias("transaction_count"),
                sum(when(transactions_df.type == "deposit", transactions_df.amount).otherwise(0)).alias("total_deposits"),
                sum(when(transactions_df.type.isin("withdrawal", "payment", "transfer"), transactions_df.amount).otherwise(0)).alias("total_debits"),
                sum(when(transactions_df.type == "deposit", transactions_df.amount).otherwise(-transactions_df.amount)).alias("net_flow")
            ).orderBy(desc("net_flow"))
            
            df_execution_time = time.time() - df_start_time
            print(f"‚è±Ô∏è Th·ªùi gian th·ª±c thi DataFrame API: {df_execution_time:.4f} gi√¢y")
            df_result.show(truncate=False)
            
            # So s√°nh k·∫øt qu·∫£ gi·ªØa 2 ph∆∞∆°ng ph√°p
            print("\nüìà So s√°nh hi·ªáu su·∫•t:")
            print(f"   SQL:          {sql_execution_time:.4f} gi√¢y")
            print(f"   DataFrame:    {df_execution_time:.4f} gi√¢y")
            print(f"   Ch√™nh l·ªách:   {abs(sql_execution_time - df_execution_time):.4f} gi√¢y")
            
            if sql_execution_time < df_execution_time:
                print(f"   üëâ SQL nhanh h∆°n {(df_execution_time/sql_execution_time - 1)*100:.2f}%")
            else:
                print(f"   üëâ DataFrame API nhanh h∆°n {(sql_execution_time/df_execution_time - 1)*100:.2f}%")
            
            # L∆∞u k·∫øt qu·∫£ ph√¢n t√≠ch v√†o PostgreSQL
            df_result.write \
                .mode("overwrite") \
                .jdbc(self.jdbc_url, "customer_transaction_summary", properties=self.connection_properties)
            
            print("‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ ph√¢n t√≠ch v√†o b·∫£ng 'customer_transaction_summary'")
            
            # ------ PH·∫¶N 3: SO S√ÅNH TRUY V·∫§N TOP 3 -------
            print("\nüìä [SQL] Top 3 giao d·ªãch l·ªõn nh·∫•t:")
            sql_top_start = time.time()
            top_transactions_query = """
                SELECT 
                    t.id as transaction_id, 
                    c.name as customer_name,
                    t.date, 
                    t.type, 
                    t.amount, 
                    t.description
                FROM transactions t
                JOIN customers c ON t.customer_id = c.id
                ORDER BY t.amount DESC
                LIMIT 3
            """
            
            sql_top_df = self.spark.sql(top_transactions_query)
            sql_top_time = time.time() - sql_top_start
            print(f"‚è±Ô∏è Th·ªùi gian th·ª±c thi SQL: {sql_top_time:.4f} gi√¢y")
            sql_top_df.show(truncate=False)
            
            print("\nüìä [DataFrame API] Top 3 giao d·ªãch l·ªõn nh·∫•t:")
            df_top_start = time.time()
            df_top = transactions_df.join(
                customers_df,
                transactions_df.customer_id == customers_df.id
            ).select(
                transactions_df.id.alias("transaction_id"),
                customers_df.name.alias("customer_name"),
                transactions_df.date,
                transactions_df.type,
                transactions_df.amount,
                transactions_df.description
            ).orderBy(desc("amount")).limit(3)
            
            df_top_time = time.time() - df_top_start
            print(f"‚è±Ô∏è Th·ªùi gian th·ª±c thi DataFrame API: {df_top_time:.4f} gi√¢y")
            df_top.show(truncate=False)
            
            # So s√°nh k·∫øt qu·∫£ gi·ªØa 2 ph∆∞∆°ng ph√°p
            print("\nüìà So s√°nh hi·ªáu su·∫•t (top 3):")
            print(f"   SQL:          {sql_top_time:.4f} gi√¢y")
            print(f"   DataFrame:    {df_top_time:.4f} gi√¢y")
            print(f"   Ch√™nh l·ªách:   {abs(sql_top_time - df_top_time):.4f} gi√¢y")
            
            if sql_top_time < df_top_time:
                print(f"   üëâ SQL nhanh h∆°n {(df_top_time/sql_top_time - 1)*100:.2f}%")
            else:
                print(f"   üëâ DataFrame API nhanh h∆°n {(sql_top_time/df_top_time - 1)*100:.2f}%")
            
        except Exception as e:
            print(f"‚ùå L·ªói khi th·ª±c hi·ªán demo join: {str(e)}")
            import traceback
            traceback.print_exc()
    
    def partition_demonstration(self):
        """Demo k·ªπ thu·∫≠t t·ªëi ∆∞u v·ªõi partitioning khi l√†m vi·ªác v·ªõi d·ªØ li·ªáu l·ªõn"""
        try:
            print("\nüöÄ Demo ph√¢n v√πng d·ªØ li·ªáu ƒë·ªÉ t·ªëi ∆∞u hi·ªáu su·∫•t...")
            
            # ƒê·ªçc d·ªØ li·ªáu giao d·ªãch t·ª´ PostgreSQL
            transactions_df = self.spark.read \
                .jdbc(self.jdbc_url, "transactions", properties=self.connection_properties)
            
            # Ph√¢n chia d·ªØ li·ªáu theo lo·∫°i giao d·ªãch
            print("\nüìä Ph√¢n v√πng d·ªØ li·ªáu theo lo·∫°i giao d·ªãch...")
            
            # L·ªçc v√† x·ª≠ l√Ω t·ª´ng lo·∫°i giao d·ªãch song song
            deposit_df = transactions_df.filter(col("type") == "deposit")
            withdrawal_df = transactions_df.filter(col("type") == "withdrawal")
            transfer_df = transactions_df.filter(col("type") == "transfer")
            payment_df = transactions_df.filter(col("type") == "payment")
            
            # ƒê·∫øm s·ªë l∆∞·ª£ng giao d·ªãch theo t·ª´ng lo·∫°i
            print(f"üìä S·ªë giao d·ªãch n·∫°p ti·ªÅn: {deposit_df.count()}")
            print(f"üìä S·ªë giao d·ªãch r√∫t ti·ªÅn: {withdrawal_df.count()}")
            print(f"üìä S·ªë giao d·ªãch chuy·ªÉn ti·ªÅn: {transfer_df.count()}")
            print(f"üìä S·ªë giao d·ªãch thanh to√°n: {payment_df.count()}")
            
            # T√≠nh t·ªïng gi√° tr·ªã giao d·ªãch theo t·ª´ng lo·∫°i
            deposit_sum = deposit_df.selectExpr("sum(amount) as total_deposit").collect()[0]["total_deposit"]
            withdrawal_sum = withdrawal_df.selectExpr("sum(amount) as total_withdrawal").collect()[0]["total_withdrawal"]
            transfer_sum = transfer_df.selectExpr("sum(amount) as total_transfer").collect()[0]["total_transfer"]
            payment_sum = payment_df.selectExpr("sum(amount) as total_payment").collect()[0]["total_payment"]
            
            print(f"üí∞ T·ªïng ti·ªÅn n·∫°p: {deposit_sum:,.2f} VND")
            print(f"üí∞ T·ªïng ti·ªÅn r√∫t: {withdrawal_sum:,.2f} VND")
            print(f"üí∞ T·ªïng ti·ªÅn chuy·ªÉn: {transfer_sum:,.2f} VND")
            print(f"üí∞ T·ªïng ti·ªÅn thanh to√°n: {payment_sum:,.2f} VND")
            
            # Demo t·ªëi ∆∞u write v·ªõi partitioning
            print("\nüìä Ghi d·ªØ li·ªáu v·ªõi partitioning theo lo·∫°i giao d·ªãch...")
            
            # Th√™m d·ªØ li·ªáu ng·∫´u nhi√™n ƒë·ªÉ demo partitioning
            transactions_df = transactions_df.withColumn("month", expr("substring(date, 6, 2)"))
            
            # Ghi v·ªõi partitioning theo th√°ng v√†o PostgreSQL
            transactions_df.write \
                .partitionBy("month") \
                .mode("overwrite") \
                .jdbc(self.jdbc_url, "transactions_partitioned", properties=self.connection_properties)
            
            print("‚úÖ ƒê√£ ghi d·ªØ li·ªáu v·ªõi partitioning!")
            
            # Demo ƒë·ªçc d·ªØ li·ªáu v·ªõi pushdown filter
            print("\nüìä Demo t·ªëi ∆∞u ƒë·ªçc d·ªØ li·ªáu v·ªõi pushdown filter...")
            
            # Thi·∫øt l·∫≠p pushdown query ƒë·ªÉ th·ª±c thi ·ªü ph√≠a database server
            pushdown_query = "(SELECT * FROM transactions WHERE amount > 5000000) AS filtered_data"
            
            start_time = time.time()
            filtered_df = self.spark.read \
                .jdbc(
                    self.jdbc_url, 
                    pushdown_query, 
                    properties=self.connection_properties
                )
            
            print(f"‚úÖ ƒê·ªçc d·ªØ li·ªáu v·ªõi pushdown filter ho√†n th√†nh trong {time.time() - start_time:.4f} gi√¢y!")
            print(f"üìä K·∫øt qu·∫£ ({filtered_df.count()} d√≤ng):")
            filtered_df.show(truncate=False)
            
        except Exception as e:
            print(f"‚ùå L·ªói khi th·ª±c hi·ªán demo partition: {str(e)}")
    
    def run_full_demo(self):
        """Ch·∫°y to√†n b·ªô demo"""
        print("\nüéÆ B·∫Øt ƒë·∫ßu ch·∫°y DEMO ƒë·∫ßy ƒë·ªß...")
        
        # T·∫°o d·ªØ li·ªáu m·∫´u
        self.create_demo_data()
        
        # Hi·ªÉn th·ªã d·ªØ li·ªáu
        self.read_data("customers")
        self.read_data("transactions")
        
        # Demo SQL Query
        self.execute_sql_query(
            """
            SELECT c.name, c.city, c.balance,
                   COUNT(t.id) as transaction_count,
                   SUM(t.amount) as total_transaction_amount
            FROM customers c
            JOIN transactions t ON c.id = t.customer_id
            GROUP BY c.name, c.city, c.balance
            ORDER BY total_transaction_amount DESC
            """,
            "customer_transaction_analysis"
        )
        
        # Demo c·∫≠p nh·∫≠t d·ªØ li·ªáu
        self.update_data("customers", "id", 1, "balance", 20000000.0)
        
        # Demo x√≥a d·ªØ li·ªáu
        # self.delete_data("transactions", "id", 110)
        
        # Demo join d·ªØ li·ªáu
        self.join_data_demonstration()
        
        # Demo partition
        self.partition_demonstration()
        
        print("\nüéâ Demo ho√†n t·∫•t!")
    
    def close(self):
        """ƒê√≥ng SparkSession"""
        if self.spark:
            self.spark.stop()
            print("‚úÖ ƒê√£ ƒë√≥ng SparkSession")


def main():
    parser = argparse.ArgumentParser(description='HDBank PySpark PostgreSQL JDBC Demo')
    parser.add_argument('--jar', type=str, 
                        default="/home/devduongthanhdat/Desktop/hdbank/hdbank_pyspark/postgresql-42.7.2.jar",
                        help='ƒê∆∞·ªùng d·∫´n t·ªõi PostgreSQL JDBC JAR')
    parser.add_argument('--db', type=str, default="hdbank_db", 
                        help='T√™n database PostgreSQL')
    parser.add_argument('--user', type=str, default="postgres", 
                        help='Username PostgreSQL')
    parser.add_argument('--password', type=str, default="hdbank123", 
                        help='M·∫≠t kh·∫©u PostgreSQL')
    
    args = parser.parse_args()
    
    try:
        # Kh·ªüi t·∫°o demo
        demo = PySparkPostgreSQLDemo(
            jdbc_jar_path=args.jar,
            db_name=args.db,
            user=args.user,
            password=args.password
        )
        
        # Ch·∫°y demo ƒë·∫ßy ƒë·ªß
        demo.run_full_demo()
        
    except Exception as e:
        print(f"‚ùå L·ªói: {str(e)}")
        
    finally:
        if 'demo' in locals():
            demo.close()

if __name__ == "__main__":
    main()