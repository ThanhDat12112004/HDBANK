# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E9oJ8gUL5qht2eEWrnEtrxpagJR1gLnK
"""

from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType
from pyspark.sql.functions import desc

spark = SparkSession.builder.appName("Ahihi").getOrCreate()
# Dữ liệu mẫu
data = [("Trần Thúy An", 21, 10), ("Huỳnh Mỹ Linh", 22, 11), ("Trần Nguyễn Bách Hợp", 21, 11),("Trần Bảo Trân", 20, 11),("Ngô Ngọc Trân", 21, 12),("Nguyễn Huyền Châm", 20, 12),("Võ Thị Cẩm Tiên",19, 15),("Sơn Thị Vĩnh Xuân",22,15),("Như Anh",19,15)]
schema = StructType([
    StructField("Name", StringType(),True),
    StructField("Age", IntegerType(),True),
    StructField("Class", IntegerType(),True),

])
df = spark.createDataFrame(data,schema)
df.show()

#Lazy Evaluation
df_filtered = df.filter(df.Age >= 20)
df_selected = df_filtered.select("Name","Age")
df_selected.show()
print(df.count())

df_cache = df.filter(df.Age >= 21).cache()
count = df_cache.count()
df_arranged = df_cache.orderBy(desc("Class"))
df_arranged.show()

df_selected.show()

# Dữ liệu mẫu
data2 = [("Trần Thúy An", 1), ("Huỳnh Mỹ Linh", 1), ("Trần Nguyễn Bách Hợp", 1),("Trần Bảo Trân", 5),("Ngô Ngọc Trân", 1),("Nguyễn Huyền Châm", 25),("Võ Thị Cẩm Tiên",5),("Sơn Thị Vĩnh Xuân",1),("Như Anh",0)]
schema2 = StructType([
    StructField("Name", StringType(),True),
    StructField("Time", IntegerType(),True)

])
df2 = spark.createDataFrame(data2,schema2)
df2.show()

df1_repartitioned = df.repartition("Name")
df2_arranged = df2.orderBy("Time").cache()
df2_repartitioned = df2_arranged.repartition("Name")
joined_df = df1_repartitioned.join(df2_repartitioned,"Name","left")
joined_df.show()
joined_df.explain()

joined_df.explain()

print(f"Số phân vùng df1_repartitioned: {df1_repartitioned.rdd.getNumPartitions()}")
print(f"Số phân vùng df2_repartitioned: {df2_repartitioned.rdd.getNumPartitions()}")

df.join(df2,"Name","inner").show()
df.join(df2,"Name","inner").explain()

spark.conf.get("spark.sql.shuffle.partitions")

from pyspark.sql.functions import broadcast
joined_df = df1_repartitioned.join(broadcast(df2_repartitioned), "Name", "right")
joined_df.show()